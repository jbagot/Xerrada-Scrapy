{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "És un framework Python que serveix per fer web scraping. Conté les eines necessàries per facilitar-nos la tasca d'obtenir dades de webs, processar-les i guardar-les."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nostre objectiu en aquesta xerrada serà escrapejar l'agenda de la PyConES que es farà al setembre.\n",
    "\n",
    "## Procés\n",
    "\n",
    "* Trobar la URL que ens retorna la informació que volem.\n",
    "* Identificar la informació que volem aconseguir.\n",
    "* Trobar els XPath.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La url on hi ha la informació és aquesta: https://2017.es.pycon.org/ca/schedule/\n",
    "\n",
    "Ara cal tenir pràctica amb la consola del navegador per entendre l'HTML i muntar els Xpaths que necessitem.\n",
    "Hi ha un plugin del Chrome que és molt útil per comprovar els Xpath, es diu: Xpath Helper: https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl\n",
    "\n",
    "També n'hi ha un per Firefox, Xpath Checker: https://addons.mozilla.org/ca/firefox/addon/xpath-checker/\n",
    "\n",
    "Amb aquests plugins podem escriure els xpaths i veurem què és el que estem recollint al moment.\n",
    "\n",
    "El procediment és buscar la informació que volem a l'HTML, veure on hi ha bucles, etc i després fer els Xpath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Què és un XPath?\n",
    "És un llenguatge que permet navegar per etiquetes mitjançant els tipus d'etiquetes i els seus atributs. Es pot utilitzar en XML o HTML. I es pot obtenir qualsevol dada que estigui dins el codi HTML.\n",
    "És important fer XPath **reutilitzables i sobretot genèrics i entendors**. Ja que es poden fer Xpath comptant divs, etc i no és una bona pràctica perquè és molt complicat seguir el fil per una altre persona o per un mateix. També és molt important que siguin genèrics i que utilitzin atributs únics o representatius com ids. Estem accedint a un codi que no és nostre i que és possible que canviï, així doncs, **si fem els Xpath genèrics i robustos no haurem de canviar-los al mínim canvi de la web**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Què és una petició i resposta HTTP?\n",
    "HTTP és el protocol que fa servir la web per comunicar-se entre els clients i els servidors.\n",
    "Un client fa una petició HTTP a una web perquè vol veure la web esperant que el servidor li retorni una resposta.\n",
    "En el nostre cas, Scrapy farà peticions a les webs que nosaltres li indiquem i rebrà la resposta on hi haurà tot el codi HTML de la web que després podrem tractar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar el projecte\n",
    "Per iniciar un projecte de scrapy hem d'executar això:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrapy startproject pycon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Això ens crearà una estructura de carpetes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pycon/\n",
    "    scrapy.cfg            \n",
    "    pycon/             \n",
    "        __init__.py\n",
    "        items.py     \n",
    "        middlewares.py\n",
    "        pipelines.py  \n",
    "        settings.py       \n",
    "        spiders/          \n",
    "            __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Com podem veure tenim:\n",
    "* Items\n",
    "* Middlewares\n",
    "* Pipelines\n",
    "* Settings\n",
    "* Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Items\n",
    "Són classes que s'utilitzen per organitzar les dades que volem guardar. És opcional, però és recomenable ja que permet que es desin les dades automàticament en JSON, etc.\n",
    "També ajuda a no fer errors de sintaxis en la sortida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middlewares\n",
    "És una classe amb certs mètodes que serveixen per manipular les peticions i respostes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "Són classes que s'executen quan es disparen certs events. Com per exemple, a l'inici d'un spider o al final.\n",
    "S'utilitzen per manipular les dades obtingudes del spider i per guardar-les en fitxers, bases de dades, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "És l'arxiu on hi ha la configuració, per exemple aquí és on es defineixen les classes dels pipelines que volem que s'activin, l'ordre, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiders\n",
    "Són les classes que cal programar perquè accedeixin a la web, recuperin el que ens interessi i es guardi.\n",
    "\n",
    "#### Funcionament dels spiders\n",
    "1. Tenen un atribut **name** amb el qual scrapy l'identifica, ha de ser únic, no pot haver-hi dos spiders amb el mateix nom.\n",
    "2. Tenen un atribut **start_urls** que és una llista de les urls a les quals accedirem.\n",
    "3. Tenen varis mètodes:\n",
    "    1. **start_requests**(opcional): serveix per modificar la petició i enviar-la amb el que s'hagi volgut canviar.\n",
    "    2. **parse**: rep per paràmetre la resposta de la petició, aquí és on hi haurà la major part de la lògica. És on hi hauran els Xpath i on obtindrem la informació que volem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Com funciona Scrapy?\n",
    "1. Hem de tenir creat els nostres spiders.\n",
    "2. Quan executem scrapy indiquem quin spider es vol executar, s'indica amb el seu **name**.\n",
    "3. El primer que s'executa del spider és el constructor.\n",
    "4. Després s'executa el mètode **start_requests** (si hi és). Si hi és generarà el Request i enviarà el Response al callback indicat. Sinó, scrapy fa la Request.\n",
    "5. Si tenim un middleware de Request l'interceptarà abans de fer la petició.\n",
    "6. Fa la petició.\n",
    "7. Si tenim un middleware de Response l'interceptarà.\n",
    "8. S'executa el mètode **parse** o el mètode del callback que hagi definit **start_requests** (si s'ha definit).\n",
    "9. Si tenim un pipelines definit s'executarà cada vegada que el mètode **parse** retorni un item. I finalment quan el spider es tanqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executar un spider"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy crawl pycon -o pycon_schedule.json\n",
    "\n",
    "# Amb un log file i amb un paràmetre\n",
    "scrapy crawl pycon -a tor=True -o pycon_schedule.json --set LOG_FILE=/home/jordi/opt/scrapy_pycon/pycon/pycon_spider.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evitar baneig de IP\n",
    "Hi ha varies maneres de fer-ho, però totes tenen el mateix objectiu, no fer peticions amb la IP real. Per això s'utilitzen proxies.\n",
    "Una manera més avançada que utilitzar un llistat estàtic de proxies és utilitzar **Tor**\n",
    "\n",
    "### Què és Tor?\n",
    "És una xarxa mundial on t'hi connectes i et genera un circuit de 5 ordinadors. Això significa que et connectes a un ordinador, aquest es connecta a un segon i així fins a 5, per tant, cada petició que facis passarà per aquest circuit i és molt complicat de restrejar qui està al final del circuit.\n",
    "Els circuits s'actualitzen, etc. Es pot configurar de moltes maneres però aquesta és la per defecte.\n",
    "\n",
    "#### Tor funciona amb SOCKS, Scrapy no ho suporta\n",
    "Tor funciona amb el protocol SOCKS i Scrapy no ho permet, només funciona amb HTTP, per tant necessitem un traductor que ens converteixi una connexió SOCKS amb una HTTP, això ho fa **Polipo** o **Privoxy**.\n",
    "\n",
    "### Instal·lació de Tor i Polipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo apt-get install tor polipo\n",
    "sudo service tor stop\n",
    "sudo service polipo stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciar Tor i Polipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tor per defecte obre el port 9050\n",
    "tor &\n",
    "# Si volem especificar un port\n",
    "mkdir data\n",
    "tor --SocksPort 9051 --DataDirectory data/tor2 &\n",
    "# Iniciem Polipo indicant-li el port de Tor i assignem el port de Polipo\n",
    "sudo polipo socksParentProxy=localhost:9050 proxyPort=14000 &\n",
    "sudo polipo socksParentProxy=localhost:9051 proxyPort=14001 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprovació d'IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl \"http://cualesmiip.es\" --proxy localhost:14000\n",
    "curl \"http://cualesmiip.es\" --proxy localhost:14001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrar Tor i Polipo a Scrapy\n",
    "El que hem de fer és que totes les peticions que es facin passin pel port de Polipo que tenim obert. Això ho farem manipulant els headers de la petició HTTP.\n",
    "Per a fer això creem un middleware que intercepti totes les Requests que es llencin i hi afegim un header proxy en el meta d'aquesta manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProxyDownloaderMiddleware(object):\n",
    "    def process_request(self, request, spider):\n",
    "        request.meta['proxy'] = 'localhost:14000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificar User-Agent\n",
    "És molt important canviar el User-Agent de les peticions que fem a través de Scrapy ja que sinó és molt fàcil veure que és un robot i no una persona qui fa les peticions. Hi ha varies maneres de fer-ho:\n",
    "* Utilitzar la configuració de Scrapy que ens posa algun tipus de User-Agent estàtic.\n",
    "* Tenir una llista de User-Agents i muntar una funció que ens en retorni un aleatoriament i posar-lo dins les peticions. Cal un manteniment per actualitzar els user-agents.\n",
    "* Utilitzar fake_user_agent, és un paquet que está a Pypi i que va actualitzant la DB de user-agents. Per mi és la millor opció. Tot i que és una llibreria més, tot depèn del projecte que volem fer, si és un projecte gran jo ho utilitzaria.\n",
    "\n",
    "#### Com posar User_Agent a les peticions\n",
    "Amb un middleware igual que el proxy, podem modificar totes les peticions afegint-hi el header del user-agent.\n",
    "Hi ha algun middleware per defecte de Scrapy de User-Agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
